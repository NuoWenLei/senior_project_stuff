CHECKED - update train sequence to record: 
	* base learner MAE
	* interpreter MAE
	* absolute sum of base learner weight (to determine if magnitude of base learner weight affects interpreter prediction)

CHECKED - create approach 6:
	* DONE Linear and Dense Combination (Wide and Deep Model)
		* see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/DNNLinearCombinedRegressor
		* see https://arxiv.org/pdf/1606.07792.pdf
	* DONE individual processing of weights + biases
	* DONE concatenate with embedding
	* DONE use batch norm
		* RESULT: Doesn't work because it doesn't globally scale weights

CHECKED - create approach 7:
	* predict angle and use cosine to determine
	* domain of cosine is 0 to pi

CHECKED - (Continue from Approach 4) Try cosine angle predict with other approaches

CHECKED - update train sequence to display:
	* DONE top 3 most similar words (UPDATE ALGO)
	* DONE similarity between most similar word and target word

- ideas:
	* DONE callback finds most similar words for every neuron as option (neuron_number = -1)
	* DONE remove words with high cosine similarity with all words
		- by calculating and ordering by summed cosine similarity of every word with all other words
		- and deleting top few
	* DONE try other end functions like sine or tangent etc
	* DONE new approach takes into account gradient
	* DONE new approach ONLY uses gradient
	* DONE new approach takes into account embedding magnitude
	* DONE train approach over night
	* DONE perhaps use cos sim between feature embeds
	* CANCEL use loss of training
		- problem with this is in multi-layer network,
		  loss will become less associated with any specific layer
	* DONE allow information of order, assign certain index to each feature and mask out the feature used as target
	* DONE Create new train sequence that includes as inputs:
		- target_col to mask out
		- covariance similarity matrix between embeddings
	* DONE Try dynamic approach (Approach 10) on salary dataset
		- compare results with static approach
		- theory:
			- static approach produces better training results but less general testing results
			- dynamic approach produces relatively same level of train and test with worse similarity overall
	* DONE Try dynamic approach (Approach 10) on housing dataset
	* Train housing dataset with model that uses word sequence embeddings instead of average embedding of words in phrase
	* Use Non-Parametric Transformer model structure:
		- https://github.com/OATML/non-parametric-transformers
		- https://arxiv.org/pdf/2106.02584.pdf

- RESULTS:
	* (4/12/2022) Currently Approach 3 is performing the best with cosine angle predict
	* (4/14/2022) cosine function is best for finalizing guesses
	* (4/19/2022) Approach 10 out-performs Approach 3, with room to improve:
		- average around 30% cosine similarity
		- similarity does not improve with training time
		- final similarity results is LOOSELY inversely correlated to loss 
	* (4/27/2022) New Approach 1 has out-performed previous approaches, showing promise in most previous problems:
		- average around 40% cosine similarity
		- similarity improves steadily with training time
		- final similarity STRONGLY inversely correlated to loss
	* (5/09/2022) Comparing the prediction results between New Approach 1 and Approach 10:
		- New Approach 1 seems to overfit more from training, predicting exact same words for most nodes
		- While Approach 10 does seem to predict more different words, it also predicts many same words for most nodes
		- I suspect nodes with same words that is present throughout different target column results are nodes that are less utilized in the network


