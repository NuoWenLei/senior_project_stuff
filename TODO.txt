CHECKED - update train sequence to record: 
	* base learner MAE
	* interpreter MAE
	* absolute sum of base learner weight (to determine if magnitude of base learner weight affects interpreter prediction)

CHECKED - create approach 6:
	* Linear and Dense Combination (Wide and Deep Model)
		* see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/DNNLinearCombinedRegressor
		* see https://arxiv.org/pdf/1606.07792.pdf
	* DONE individual processing of weights + biases
	* DONE concatenate with embedding
	* DONE use batch norm
		* RESULT: Doesn't work because it doesn't globally scale weights

CHECKED - create approach 7:
	* predict angle and use cosine to determine
	* domain of cosine is 0 to pi

- (Continue from Approach 4) Try cosine angle predict with other approaches

- CHECKED update train sequence to display:
	* DONE top 3 most similar words (UPDATE ALGO)
	* DONE similarity between most similar word and target word

- ideas:
	* DONE callback finds most similar words for every neuron as option (neuron_number = -1)
	* DONE remove words with high cosine similarity with all words
		- by calculating and ordering by summed cosine similarity of every word with all other words
		- and deleting top few
	* DONE try other end functions like sine or tangent etc
	* DONE new approach takes into account gradient
	* DONE new approach ONLY uses gradient
	* DONE new approach takes into account embedding magnitude
	* DONE train approach over night
	* NEED TEST perhaps use cos sim between feature embeds
	* use loss of training
		- problem with this is in multi-layer network,
		  loss will become less associated with any specific layer
	* NEED TEST allow information of order, assign certain index to each feature and mask out the feature used as target
	* Create new train sequence that includes as inputs:
		- target_col to mask out
		- covariance similarity matrix between embeddings

- RESULTS:
	* Currently Approach 3 is performing the best with cosine angle predict
	* cosine function is best for finalizing guesses


