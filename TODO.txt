CHECKED - update train sequence to record: 
	* base learner MAE
	* interpreter MAE
	* absolute sum of base learner weight (to determine if magnitude of base learner weight affects interpreter prediction)

CHECKED - create approach 6:
	* Linear and Dense Combination (Wide and Deep Model)
		* see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/DNNLinearCombinedRegressor
		* see https://arxiv.org/pdf/1606.07792.pdf
	* DONE individual processing of weights + biases
	* DONE concatenate with embedding
	* perhaps use cos sim between feature embeds
	* DONE use batch norm
		* RESULT: Doesn't work because it doesn't globally scale weights

- create approach 7:
	* predict angle and use cosine to determine
	* domain of cosine is 0 to pi

- update train sequence to display:
	* top 3 most similar words (UPDATE ALGO)
	* similarity between most similar word and target word
	* similarity between predicted embedding and target word embedding

- ideas:
	* new loss function that uses difference between predicted embedding and target word embedding
	* callback finds most similar words for every neuron as option (neuron_number = -1)
	* remove words with high cosine similarity with all words
		- by calculating and ordering by summed cosine similarity of every word with all other words
		- and deleting top few



